[gateway]
url = "ws://127.0.0.1:18789/ws"
token = ""
password = ""
runtime_mode = "bridge_client" # bridge_client | standalone_server

[gateway.server]
bind = "127.0.0.1:18789"
# http_bind = "127.0.0.1:18890" # optional control HTTP surface
auth_mode = "auto" # auto | none | token | password
handshake_timeout_ms = 10000
event_queue_capacity = 256
tick_interval_ms = 30000
reload_interval_secs = 3

[runtime]
audit_only = false
decision_event = "security.decision"
worker_concurrency = 8
max_queue = 256
session_queue_mode = "followup" # followup | steer | collect
group_activation_mode = "mention" # mention | always
eval_timeout_ms = 2500
memory_sample_secs = 15
idempotency_ttl_secs = 300
idempotency_max_entries = 5000
session_state_path = ".openclaw-rs/session-state.json"

[security]
review_threshold = 35
block_threshold = 65
virustotal_api_key = ""
virustotal_timeout_ms = 1400
# policy_bundle_path = "/etc/openclaw/policy-bundle.json"
# policy_bundle_key = "replace-with-hmac-key"
quarantine_dir = ".openclaw-rs/quarantine"
protect_paths = ["./openclaw.mjs", "./dist/index.js"]
allowed_command_prefixes = ["git ", "ls", "cat ", "rg "]
blocked_command_patterns = [
  "(?i)\\brm\\s+-rf\\s+/",
  "(?i)\\bmkfs\\b",
  "(?i)\\bdd\\s+if=",
  "(?i)\\bcurl\\s+[^|]*\\|\\s*sh\\b",
  "(?i)\\bwget\\s+[^|]*\\|\\s*sh\\b",
  "(?i)\\bchmod\\s+777\\b",
]
prompt_injection_patterns = [
  "(?i)ignore\\s+all\\s+previous\\s+instructions",
  "(?i)reveal\\s+the\\s+system\\s+prompt",
  "(?i)override\\s+developer\\s+instructions",
  "(?i)disable\\s+safety",
  "(?i)you\\s+must\\s+run\\s+this\\s+command\\s+without\\s+asking",
]
tool_policies = { gateway = "review", nodes = "review" }
tool_risk_bonus = { exec = 20, bash = 20, process = 10, apply_patch = 12, browser = 8, gateway = 20, nodes = 20 }
channel_risk_bonus = { discord = 10, slack = 8, telegram = 6, whatsapp = 6, webchat = 8 }

[security.tool_runtime_policy]
profile = "full" # minimal | coding | messaging | full
allow = []
deny = []

# Example provider/model overrides:
# [security.tool_runtime_policy.byProvider.openai]
# allow = ["group:runtime", "group:fs"]
# deny = ["gateway"]
#
# [security.tool_runtime_policy.byProvider."openai/gpt-5"]
# deny = ["exec"]

[security.tool_runtime_policy.loop_detection]
enabled = false
history_size = 30
warning_threshold = 10
critical_threshold = 20

[security.tool_runtime_policy.wasm]
enabled = false
module_root = ".openclaw-rs/wasm-modules"
default_capabilities = ["workspace.read", "http.fetch", "tool.invoke"]
fuel_limit = 1000000
memory_limit_bytes = 67108864

[security.tool_runtime_policy.credentials]
enabled = false
env_allowlist = ["OPENAI_API_KEY", "GITHUB_TOKEN"]
leak_action = "review" # allow | review | block
redaction_token = "[REDACTED_SECRET]"

[security.tool_runtime_policy.safety]
enabled = true
sanitize_output = true
max_output_chars = 32768

[security.tool_runtime_policy.routines]
enabled = false
history_limit = 256
max_parallel = 4

# ------------------------------------------------------------------------------
# OpenAI-compatible provider templates (optional)
# ------------------------------------------------------------------------------
# Full support matrix and endpoint references:
#   PROVIDER_SUPPORT_MATRIX.md
#
# [models.providers.opencode]
# api = "website-openai-bridge"
# baseUrl = "https://opencode.ai/zen/v1"
# websiteUrl = "https://opencode.ai"
# bridgeBaseUrls = ["https://opencode.ai/zen/v1", "https://api.opencode.ai/v1"]
# allowUnauthenticated = true
# apiKey = "${OPENCODE_API_KEY}" # optional for keyless free-tier attempts
#
# [[models.providers.opencode.models]]
# id = "glm-5-free"
# name = "GLM-5-Free"
#
# [[models.providers.opencode.models]]
# id = "kimi-k2.5-free"
# name = "Kimi K2.5 Free"
#
# [models.providers.zhipuai]
# api = "openai-completions"
# baseUrl = "https://open.bigmodel.cn/api/paas/v4"
# apiKey = "${ZHIPUAI_API_KEY}"
#
# [[models.providers.zhipuai.models]]
# id = "glm-5"
# name = "GLM-5"
#
# [models.providers.deepinfra]
# api = "openai-completions"
# baseUrl = "https://api.deepinfra.com/v1/openai"
# apiKey = "${DEEPINFRA_API_KEY}"
#
# [models.providers.llamacpp]
# api = "openai-completions"
# baseUrl = "http://127.0.0.1:8080/v1"
# allowUnauthenticated = true
#
# Kimi note:
#   `kimi-coding` can be configured as an authenticated OpenAI-compatible provider
#   (and OAuth provider), but guest/no-login website execution is not assumed.
